{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "from data_utils import load_task, vectorize_data\n",
    "import sklearn\n",
    "from memn2n import MemN2N\n",
    "from itertools import chain\n",
    "from six.moves import range, reduce\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-992305738b02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'kernel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Task: 1\n"
     ]
    }
   ],
   "source": [
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.01, \"Learning rate for SGD.\")\n",
    "tf.app.flags.DEFINE_float(\"anneal_rate\", 25, \"Number of epochs between halving the learnign rate.\")\n",
    "tf.app.flags.DEFINE_float(\"anneal_stop_epoch\", 100, \"Epoch number to end annealed lr schedule.\")\n",
    "tf.app.flags.DEFINE_float(\"max_grad_norm\", 40.0, \"Clip gradients to this norm.\")\n",
    "tf.app.flags.DEFINE_integer(\"evaluation_interval\", 10, \"Evaluate and print results every x epochs\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 32, \"Batch size for training.\")\n",
    "tf.app.flags.DEFINE_integer(\"hops\", 3, \"Number of hops in the Memory Network.\")\n",
    "tf.app.flags.DEFINE_integer(\"epochs\", 100, \"Number of epochs to train for.\")\n",
    "tf.app.flags.DEFINE_integer(\"embedding_size\", 20, \"Embedding size for embedding matrices.\")\n",
    "tf.app.flags.DEFINE_integer(\"memory_size\", 50, \"Maximum size of memory.\")\n",
    "tf.app.flags.DEFINE_integer(\"task_id\", 1, \"bAbI task id, 1 <= id <= 20\")\n",
    "tf.app.flags.DEFINE_integer(\"random_state\", None, \"Random state.\")\n",
    "tf.app.flags.DEFINE_string(\"data_dir\", \"data/tasks_1-20_v1-2/en/\", \"Directory containing bAbI tasks\")\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "print(\"Started Task:\", FLAGS.task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD AND PROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = open(\"train_bodies.txt\", \"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = open(\"train_claims.txt\", \"r\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "0 - unrelated\n",
    "1 - discussed\n",
    "2 - agree\n",
    "3 - disagree\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"0 a small meteorite crashed into a wooded area in nicaragua 's capital of managua overnight the government said sunday residents reported hearing a mysterious boom that left a <number>-foot deep crater near the city 's airport the associated press reports\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'712 police find mass graves with at least <number> bodies  near mexico town where <number> students disappeared after police clash 0\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_proc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in x:\n",
    "    s_proc = s.split(\" \")\n",
    "    s_init = []\n",
    "    sent = ' '.join(s_proc[1:])\n",
    "    sent = sent.split(\"\\n\")[0]\n",
    "    s_init.extend([s_proc[0], sent])\n",
    "    x_proc.append(s_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " \"a small meteorite crashed into a wooded area in nicaragua 's capital of managua overnight the government said sunday residents reported hearing a mysterious boom that left a <number>-foot deep crater near the city 's airport the associated press reports\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_proc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = np.array(x_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1683"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(paras[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proc = []\n",
    "for s in y:\n",
    "    s_proc = s.split(\"\\n\")[0].split(\" \")\n",
    "    s_init = []\n",
    "    s_init.extend([s_proc[0], ' '.join(s_proc[1:-1]), s_proc[-1]])\n",
    "    y_proc.append(s_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['712',\n",
       " 'police find mass graves with at least <number> bodies  near mexico town where <number> students disappeared after police clash',\n",
       " '0']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = np.array(y_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(claims[:,0])) == len(np.unique(paras[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "claimsdf = pd.DataFrame(claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parasdf = pd.DataFrame(paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>a small meteorite crashed into a wooded area i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>government spokeswoman rosario murillo said a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>the crater left by the meteorite had a radius ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>humberto garcia of the astronomy center at the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>we have to study it more because it could be i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0  0  a small meteorite crashed into a wooded area i...\n",
       "1  0  government spokeswoman rosario murillo said a ...\n",
       "2  0  the crater left by the meteorite had a radius ...\n",
       "3  0  humberto garcia of the astronomy center at the...\n",
       "4  0  we have to study it more because it could be i..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parasdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>712</td>\n",
       "      <td>police find mass graves with at least &lt;number&gt;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>158</td>\n",
       "      <td>hundreds of palestinians flee floods in gaza a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137</td>\n",
       "      <td>christian bale passes on role of steve jobs ac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1034</td>\n",
       "      <td>hbo and apple in talks for &lt;number&gt;month apple...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1923</td>\n",
       "      <td>spider burrowed through tourist 's stomach and...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1  2\n",
       "0   712  police find mass graves with at least <number>...  0\n",
       "1   158  hundreds of palestinians flee floods in gaza a...  2\n",
       "2   137  christian bale passes on role of steve jobs ac...  0\n",
       "3  1034  hbo and apple in talks for <number>month apple...  0\n",
       "4  1923  spider burrowed through tourist 's stomach and...  3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claimsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ours = []\n",
    "\n",
    "dict_stances = {\n",
    "    '0': \"unrelated\",\n",
    "    '1': \"discussed\",\n",
    "    '2': \"agree\",\n",
    "    '3': \"disagree\"\n",
    "}\n",
    "\n",
    "for bodyid in parasdf[0].unique():\n",
    "    paras_with_this_id = list(parasdf[parasdf[0]==bodyid][1].values)\n",
    "    paras_with_this_id = [p.split() for p in paras_with_this_id]\n",
    "    claims_for_this_id = claimsdf[claimsdf[0]==bodyid]\n",
    "    for i, claim in claims_for_this_id.iterrows():\n",
    "        entity = []\n",
    "        entity.append(list(paras_with_this_id))\n",
    "        entity.append(claim[1].split(\" \"))\n",
    "        claim_int = claim[2]\n",
    "        claim_text = dict_stances[claim_int]\n",
    "        entity.append([claim_text])\n",
    "        ours.append(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOCAB, VECTORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(reduce(lambda x, y: x | y, (set(list(chain.from_iterable(s)) + q + a) for s, q, a in data)))\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_story_size = max(map(len, (s for s, _, _ in data)))\n",
    "mean_story_size = int(np.mean([ len(s) for s, _, _ in data ]))\n",
    "sentence_size = max(map(len, chain.from_iterable(s for s, _, _ in data)))\n",
    "query_size = max(map(len, (q for _, q, _ in data)))\n",
    "# memory_size = min(FLAGS.memory_size, max_story_size)\n",
    "memory_size = max_story_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time words/indexes\n",
    "for i in range(memory_size):\n",
    "    word_idx['time{}'.format(i+1)] = 'time{}'.format(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence length 797\n",
      "Longest story length 9\n",
      "Average story length 6\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_idx) + 1 # +1 for nil word\n",
    "sentence_size = max(query_size, sentence_size) # for the position\n",
    "sentence_size += 1  # +1 for time words\n",
    "\n",
    "print(\"Longest sentence length\", sentence_size)\n",
    "print(\"Longest story length\", max_story_size)\n",
    "print(\"Average story length\", mean_story_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/validation/test sets\n",
    "S, Q, A = vectorize_data(data, word_idx, sentence_size, memory_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainS, testS, trainQ, testQ, trainA, testA = cross_validation.train_test_split(S, Q, A, test_size=.2, random_state=FLAGS.random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the full dataset into a train and test. Then we split the test again to obtain a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "testS, valS, testQ, valQ, testA, valA = cross_validation.train_test_split(testS, testQ, testA, test_size=.5, random_state=FLAGS.random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  145 16833  8254 ...     0     0 20438]\n",
      " [ 2828 13054  9519 ...     0     0 20437]\n",
      " [ 2829 11214  9519 ...     0     0 20436]\n",
      " ...\n",
      " [    0     0     0 ...     0     0     0]\n",
      " [    0     0     0 ...     0     0     0]\n",
      " [    0     0     0 ...     0     0     0]]\n",
      "Training set shape (4000, 9, 797)\n",
      "Training Size 4000\n",
      "Validation Size 500\n",
      "Testing Size 500\n"
     ]
    }
   ],
   "source": [
    "print(testS[0])\n",
    "\n",
    "print(\"Training set shape\", trainS.shape)\n",
    "\n",
    "# params\n",
    "n_train = trainS.shape[0]\n",
    "n_test = testS.shape[0]\n",
    "n_val = valS.shape[0]\n",
    "\n",
    "print(\"Training Size\", n_train)\n",
    "print(\"Validation Size\", n_val)\n",
    "print(\"Testing Size\", n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.argmax(trainA, axis=1)\n",
    "test_labels = np.argmax(testA, axis=1)\n",
    "val_labels = np.argmax(valA, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(FLAGS.random_state)\n",
    "batch_size = FLAGS.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = zip(range(0, n_train-batch_size, batch_size), range(batch_size, n_train, batch_size))\n",
    "batches = [(start, end) for start, end in batches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    model = MemN2N(batch_size, vocab_size, sentence_size, memory_size, \n",
    "                   FLAGS.embedding_size, \n",
    "                   session=sess,\n",
    "                   hops=FLAGS.hops, \n",
    "                   max_grad_norm=FLAGS.max_grad_norm)\n",
    "    for t in range(1, FLAGS.epochs+1):\n",
    "        # Stepped learning rate\n",
    "        if t - 1 <= FLAGS.anneal_stop_epoch:\n",
    "            anneal = 2.0 ** ((t - 1) // FLAGS.anneal_rate)\n",
    "        else:\n",
    "            anneal = 2.0 ** (FLAGS.anneal_stop_epoch // FLAGS.anneal_rate)\n",
    "        lr = FLAGS.learning_rate / anneal\n",
    "\n",
    "        np.random.shuffle(batches)\n",
    "        total_cost = 0.0\n",
    "        for start, end in batches:\n",
    "            s = trainS[start:end]\n",
    "            q = trainQ[start:end]\n",
    "            a = trainA[start:end]\n",
    "            cost_t = model.batch_fit(s, q, a, lr)\n",
    "            total_cost += cost_t\n",
    "\n",
    "        if t % FLAGS.evaluation_interval == 0:\n",
    "            train_preds = []\n",
    "            for start in range(0, n_train, batch_size):\n",
    "                end = start + batch_size\n",
    "                s = trainS[start:end]\n",
    "                q = trainQ[start:end]\n",
    "                pred = model.predict(s, q)\n",
    "                train_preds += list(pred)\n",
    "\n",
    "            val_preds = model.predict(valS, valQ)\n",
    "            train_acc = metrics.accuracy_score(np.array(train_preds), train_labels)\n",
    "            val_acc = metrics.accuracy_score(val_preds, val_labels)\n",
    "\n",
    "            print('-----------------------')\n",
    "            print('Epoch', t)\n",
    "            print('Total Cost:', total_cost)\n",
    "            print('Training Accuracy:', train_acc)\n",
    "            print('Validation Accuracy:', val_acc)\n",
    "            print('-----------------------')\n",
    "\n",
    "    test_preds = model.predict(testS, testQ)\n",
    "    test_acc = metrics.accuracy_score(test_preds, test_labels)\n",
    "    print(\"Testing Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20443"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_idx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {v:k for k,v in word_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "20441",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-feaefb44ab01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0midx2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-60-feaefb44ab01>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0midx2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 20441"
     ]
    }
   ],
   "source": [
    "[idx2word[k] for k in trainS[0][0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
