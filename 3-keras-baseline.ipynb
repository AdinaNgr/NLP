{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "import numpy as np\n",
    "\n",
    "from dataproc_utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "random_state = 42\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "body_size = 20\n",
    "claim_size = 12\n",
    "embedding_dim = 100\n",
    "output_size = 4  # size of the output vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First input tuple (body, claim, stance):\n",
      " (['seth', 'rogen', 'is', 'being', 'eyed', 'to', 'play', 'apple', 'co-founder', 'steve', 'wozniak', 'in', 'sonys', 'steve', 'jobs', 'biopic', 'danny', 'boyle', 'is', 'directing', 'the', 'untitled', 'film', 'based', 'on', 'walter', 'isaacson', \"'s\", 'book', 'and', 'adapted', 'by', 'aaron', 'sorkin', 'which', 'is', 'one', 'of', 'the', 'most', 'anticipated', 'biopics', 'in', 'recent', 'years', 'negotiations', 'have', 'not', 'yet', 'begun', 'and', 'its', 'not', 'even', 'clear', 'if', 'rogen', 'has', 'an', 'official', 'offer', 'but', 'the', 'producers', 'scott', 'rudin', 'guymon', 'casady', 'and', 'mark', 'gordon', 'have', 'set', 'their', 'sights', 'on', 'the', 'talent', 'and', 'are', 'in', 'talks', 'of', 'course', 'this', 'may', 'all', 'be', 'for', 'naught', 'as', 'christian', 'bale', 'the', 'actor', 'who', 'is', 'to', 'play', 'jobs', 'is', 'still', 'in', 'the', 'midst', 'of', 'closing', 'his', 'deal', 'sources', 'say', 'that', 'dealmaking', 'process', 'is', 'in', 'a', 'sensitive', 'stage', 'insiders', 'say', 'boyle', 'will', 'is', 'flying', 'to', 'los', 'angeles', 'to', 'meet', 'with', 'actress', 'to', 'play', 'one', 'of', 'the', 'female', 'leads', 'an', 'assistant', 'to', 'jobs', 'insiders', 'say', 'that', 'jessica', 'chastain', 'is', 'one', 'of', 'the', 'actresses', 'on', 'the', 'meeting', 'list', 'wozniak', 'known', 'as', 'woz', 'co-founded', 'apple', 'with', 'jobs', 'and', 'ronald', 'wayne', 'he', 'first', 'met', 'jobs', 'when', 'they', 'worked', 'at', 'atari', 'and', 'later', 'was', 'responsible', 'for', 'creating', 'the', 'early', 'apple', 'computers'], ['police', 'find', 'mass', 'graves', 'with', 'at', 'least', '<number>', 'bodies', 'near', 'mexico', 'town', 'where', '<number>', 'students', 'disappeared', 'after', 'police', 'clash'], 0)\n",
      "Vocab size: 3428 unique words in the train set\n",
      "Training set shape: bodies (116848, 20)\n",
      "Training set shape: claims (116848, 12)\n",
      "Training set shape: labels (116848,)\n",
      "Training Size 116848\n",
      "Validation Size 9995\n"
     ]
    }
   ],
   "source": [
    "# load data and labels\n",
    "data = load_proc_data('train_bodies.txt', 'train_claims.txt', split_pars=False)\n",
    "labels = [label for body, claim, label in data]\n",
    "y = np.array(labels)\n",
    "\n",
    "print('First input tuple (body, claim, stance):\\n', data[0])\n",
    "\n",
    "\n",
    "# train/validation/test split\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(data, y, test_size=.2, random_state=random_state)\n",
    "\n",
    "\n",
    "# create a vocabulary dict from train data\n",
    "word2freq = make_word_freq_V(train_data)\n",
    "word2index = word2idx(word2freq)\n",
    "\n",
    "vocab_size = len(word2index) + 1\n",
    "print('Vocab size:', vocab_size, 'unique words in the train set')\n",
    "\n",
    "# vectorize input words (turn each word into its index from the word2index dict)\n",
    "# for new words in test set that don't appear in train set, use index of <unknown>\n",
    "train_body, train_claim = word_vectorizer(train_data, word2index, max_body_len=body_size, max_claim_len=claim_size)\n",
    "val_body, val_claim = word_vectorizer(val_data, word2index, max_body_len=body_size, max_claim_len=claim_size)\n",
    "\n",
    "\n",
    "# perform random under/over sampling to prevent class imbalance\n",
    "train_body, train_claim, train_labels = random_sampler(train_body, train_claim, train_labels, type='over')\n",
    "\n",
    "\n",
    "print(\"Training set shape: bodies\", train_body.shape)\n",
    "print(\"Training set shape: claims\", train_claim.shape)\n",
    "print(\"Training set shape: labels\", train_labels.shape)\n",
    "\n",
    "# data size params\n",
    "n_train = train_body.shape[0]\n",
    "n_val = val_body.shape[0]\n",
    "\n",
    "print(\"Training Size\", n_train)\n",
    "print(\"Validation Size\", n_val)\n",
    "\n",
    "\n",
    "# initialize placeholders\n",
    "input_body = Input((body_size,))\n",
    "input_claim = Input((claim_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM !!!!\n",
    "# encoders\n",
    "# embed the input sequence into a sequence of vectors\n",
    "body_encoder_m = Sequential()\n",
    "body_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=embedding_dim))\n",
    "body_encoder_m.add(Dropout(0.3))  # output: (samples, body_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN!!!!!\n",
    "# embed the input into a sequence of vectors of size claim_size\n",
    "body_encoder_c = Sequential()\n",
    "body_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                             output_dim=claim_size))\n",
    "body_encoder_c.add(Dropout(0.3))  # output: (samples, body_size, claim_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the claim into a sequence of vectors\n",
    "claim_encoder = Sequential()\n",
    "claim_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=claim_size))\n",
    "claim_encoder.add(Dropout(0.3))  # output: (samples, claim_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode article bodies and claims (which are indices) as sequences of dense vectors\n",
    "body_encoded_m = body_encoder_m(input_body)\n",
    "body_encoded_c = body_encoder_c(input_body)\n",
    "claim_encoded = claim_encoder(input_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a 'match' between the body vector sequence and the claim vector sequence\n",
    "match = dot([body_encoded_m, claim_encoded], axes=(2, 2))\n",
    "match = Activation('softmax')(match)  # shape: (samples, body_size, claim_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, body_encoded_c])  # (samples, body_size, claim_size)\n",
    "response = Permute((2, 1))(response)  # (samples, claim_size, body_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the match matrix with the claim vector sequence\n",
    "stance = concatenate([response, claim_encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original paper uses a matrix multiplication for this reduction step.\n",
    "# we choose to use a RNN instead.\n",
    "stance = LSTM(32)(stance)  # shape: (samples, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one regularization layer -- more would probably be needed.\n",
    "stance = Dropout(0.3)(stance)\n",
    "stance = Dense(output_size)(stance)  # (samples, output_size)\n",
    "# we output a probability distribution over the four stances\n",
    "preds = Activation('softmax')(stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 116848 samples, validate on 9995 samples\n",
      "Epoch 1/25\n",
      "116848/116848 [==============================] - 50s 425us/step - loss: 0.7885 - acc: 0.6523 - val_loss: 1.0318 - val_acc: 0.4723\n",
      "Epoch 2/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.5505 - acc: 0.7768 - val_loss: 0.7219 - val_acc: 0.6737\n",
      "Epoch 3/25\n",
      "116848/116848 [==============================] - 46s 393us/step - loss: 0.4492 - acc: 0.8250 - val_loss: 0.6841 - val_acc: 0.6996\n",
      "Epoch 4/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.3910 - acc: 0.8507 - val_loss: 0.7181 - val_acc: 0.6965\n",
      "Epoch 5/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.3499 - acc: 0.8702 - val_loss: 0.6208 - val_acc: 0.7552\n",
      "Epoch 6/25\n",
      "116848/116848 [==============================] - 46s 393us/step - loss: 0.3152 - acc: 0.8850 - val_loss: 0.6199 - val_acc: 0.7593\n",
      "Epoch 7/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.2830 - acc: 0.8976 - val_loss: 0.6020 - val_acc: 0.7813\n",
      "Epoch 8/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.2614 - acc: 0.9078 - val_loss: 0.5360 - val_acc: 0.8104\n",
      "Epoch 9/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.2406 - acc: 0.9161 - val_loss: 0.5055 - val_acc: 0.8293\n",
      "Epoch 10/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.2240 - acc: 0.9233 - val_loss: 0.5014 - val_acc: 0.8401\n",
      "Epoch 11/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.2106 - acc: 0.9288 - val_loss: 0.4896 - val_acc: 0.8462\n",
      "Epoch 12/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.1988 - acc: 0.9328 - val_loss: 0.4976 - val_acc: 0.8492\n",
      "Epoch 13/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.1896 - acc: 0.9364 - val_loss: 0.4739 - val_acc: 0.8527\n",
      "Epoch 14/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.1814 - acc: 0.9392 - val_loss: 0.5164 - val_acc: 0.8409\n",
      "Epoch 15/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.1733 - acc: 0.9423 - val_loss: 0.4643 - val_acc: 0.8649\n",
      "Epoch 16/25\n",
      "116848/116848 [==============================] - 46s 393us/step - loss: 0.1675 - acc: 0.9448 - val_loss: 0.4603 - val_acc: 0.8658\n",
      "Epoch 17/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.1608 - acc: 0.9466 - val_loss: 0.4585 - val_acc: 0.8680\n",
      "Epoch 18/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.1572 - acc: 0.9477 - val_loss: 0.4554 - val_acc: 0.8734\n",
      "Epoch 19/25\n",
      "116848/116848 [==============================] - 46s 394us/step - loss: 0.1517 - acc: 0.9503 - val_loss: 0.4798 - val_acc: 0.8666\n",
      "Epoch 20/25\n",
      "116848/116848 [==============================] - 46s 393us/step - loss: 0.1474 - acc: 0.9509 - val_loss: 0.4569 - val_acc: 0.8758\n",
      "Epoch 21/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.1443 - acc: 0.9528 - val_loss: 0.4596 - val_acc: 0.8751\n",
      "Epoch 22/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.1406 - acc: 0.9536 - val_loss: 0.4455 - val_acc: 0.8785\n",
      "Epoch 23/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.1386 - acc: 0.9543 - val_loss: 0.4524 - val_acc: 0.8845\n",
      "Epoch 24/25\n",
      "116848/116848 [==============================] - 46s 393us/step - loss: 0.1363 - acc: 0.9552 - val_loss: 0.4786 - val_acc: 0.8717\n",
      "Epoch 25/25\n",
      "116848/116848 [==============================] - 46s 392us/step - loss: 0.1332 - acc: 0.9566 - val_loss: 0.4662 - val_acc: 0.8804\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 12)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       multiple             342800      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 12, 100)      342800      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 20, 12)       0           sequential_1[1][0]               \n",
      "                                                                 sequential_3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 12)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       multiple             41136       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 20, 12)       0           activation_1[0][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 12, 20)       0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 12, 120)      0           permute_1[0][0]                  \n",
      "                                                                 sequential_3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 32)           19584       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32)           0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            132         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 4)            0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 746,452\n",
      "Trainable params: 746,452\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = Model([input_body, input_claim], preds)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# train\n",
    "model.fit([train_body, train_claim], train_labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=([val_body, val_claim], val_labels))\n",
    "\n",
    "# print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
